{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB-9BZVuzHNO",
        "outputId": "85c04f26-43c7-4361-b0f3-817d1bc6bc49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_path = '/content/drive/MyDrive/best_model.weights.h5'"
      ],
      "metadata": {
        "id": "bEaiYMQuzVVF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def build_shakespeare_model(batch_size=None):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(None,), batch_size=batch_size),\n",
        "        tf.keras.layers.Embedding(input_dim=98, output_dim=384),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.GRU(1536, return_sequences=True,\n",
        "                          dropout=0.3, recurrent_dropout=0.3,\n",
        "                          stateful=batch_size is not None),\n",
        "        tf.keras.layers.GRU(768, return_sequences=True,\n",
        "                          dropout=0.2,\n",
        "                          stateful=batch_size is not None),\n",
        "        tf.keras.layers.Dense(98)\n",
        "    ])\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA2PJkNeyQKQ",
        "outputId": "496cfc52-20bf-4cfc-f0cb-882b316176a6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize for inference (batch_size=1)\n",
        "model = build_shakespeare_model(batch_size=1)\n",
        "model.load_weights(weights_path)"
      ],
      "metadata": {
        "id": "GPiwPgN144OW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test input matching your vocabulary\n",
        "# Using actual character IDs from your mappings:\n",
        "test_input = tf.constant([[6, 6, 6, 0, 41, 42, 23, 40, 42, 0]])  # From your sample conversion\n",
        "print(\"Test output shape:\", model(test_input).shape)  # Should output (1, 10, 98)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWEK6Mdi47Cr",
        "outputId": "67374bb5-c5b3-4ae3-b0dd-2be972829985"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test output shape: (1, 10, 98)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/shakespeare_model.keras')  # New .keras format\n",
        "\n",
        "# 5. (Optional) Save in HDF5 format if needed\n",
        "model.save('/content/drive/MyDrive/shakespeare_model.h5')  # Legacy format"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LiruaQ-4-7M",
        "outputId": "3ea067df-2ada-440b-e516-5d2b4c412f85"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model to verify\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/shakespeare_model.keras')\n",
        "\n",
        "# Test with sample input\n",
        "test_input = tf.constant([[6, 6, 6, 0, 41, 42, 23, 40, 42, 0]])\n",
        "print(loaded_model(test_input).shape)  # Should output (1, 10, 98)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUARzfRk5pXC",
        "outputId": "14afeb54-362f-4cb2-9fc3-300c86bf10ed"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 10, 98)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# Check available accelerators\n",
        "print(\"Available devices:\")\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "# Set up multi-GPU if available\n",
        "strategy = tf.distribute.MirroredStrategy() if len(tf.config.list_physical_devices('GPU')) > 1 else tf.distribute.get_strategy()\n",
        "print(f\"Using strategy: {strategy.__class__.__name__}\")\n",
        "\n",
        "# Download Shakespeare dataset\n",
        "!mkdir -p data\n",
        "!wget https://www.gutenberg.org/files/100/100-0.txt -O data/shakespeare.txt\n",
        "def clean_text(text):\n",
        "    \"\"\"More careful cleaning that maintains consistency\"\"\"\n",
        "    text = text.replace('\\r', ' ')  # Replace with space instead of removing\n",
        "    text = text.replace('\\n', ' ')  # Replace with space instead of removing\n",
        "    text = ' '.join(text.split())  # Normalize spaces/\n",
        "    return text\n",
        "\n",
        "# 1. Load and clean text\n",
        "with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = clean_text(f.read())\n",
        "print(f\"Loaded text length: {len(text)} characters\")\n",
        "\n",
        "# 2. Create vocabulary from CLEANED text\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(\"Sample mappings:\")\n",
        "for char in list(vocab)[:5]:\n",
        "    print(f\"'{char}': {char2idx[char]}\")\n",
        "\n",
        "# 3. Convert text to integers\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "print(f\"Text as integers shape: {text_as_int.shape}\")\n",
        "print(\"Sample conversion:\", text_as_int[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1M2PUMk9GgY",
        "outputId": "ea65ff49-539c-4144-f8ee-03aa83547b29"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available devices:\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 15266377498696355182\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14619377664\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 5401738242019881787\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "xla_global_id: 416903419\n",
            "]\n",
            "Using strategy: _DefaultDistributionStrategy\n",
            "--2025-08-02 18:04:00--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5618733 (5.4M) [text/plain]\n",
            "Saving to: ‘data/shakespeare.txt’\n",
            "\n",
            "data/shakespeare.tx 100%[===================>]   5.36M  9.07MB/s    in 0.6s    \n",
            "\n",
            "2025-08-02 18:04:01 (9.07 MB/s) - ‘data/shakespeare.txt’ saved [5618733/5618733]\n",
            "\n",
            "Loaded text length: 5297571 characters\n",
            "Vocabulary size: 98\n",
            "Sample mappings:\n",
            "' ': 0\n",
            "'!': 1\n",
            "'&': 2\n",
            "''': 3\n",
            "'(': 4\n",
            "Text as integers shape: (5297571,)\n",
            "Sample conversion: [ 6  6  6  0 41 42 23 40 42  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and clean text again\n",
        "def clean_text(text):\n",
        "    text = text.replace('\\r', ' ')\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = clean_text(f.read())\n",
        "\n",
        "\n",
        "# Recreate vocab\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "vocab_size = len(vocab)\n"
      ],
      "metadata": {
        "id": "80189bOQ4h6u"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define this once in your notebook/script\n",
        "def reset_rnn_states(model):\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'reset_states'):\n",
        "            layer.reset_states()\n"
      ],
      "metadata": {
        "id": "e24y_qR7907M"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string, char2idx, idx2char, num_generate=1000, temperature=1.0):\n",
        "    input_eval = [char2idx[c] for c in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    reset_rnn_states(model)  # Reset RNN states properly\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[:, -1, :] / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n"
      ],
      "metadata": {
        "id": "8EQjcksx9gjC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated = generate_text(\n",
        "    model=model,\n",
        "    start_string=\"What light through yonder window breaks\",\n",
        "    char2idx=char2idx,\n",
        "    idx2char=idx2char,\n",
        "    num_generate=1000,\n",
        "    temperature=0.8  # Lower = more predictable, higher = more creative\n",
        ")\n",
        "\n",
        "print(generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNPl8QNL9jPb",
        "outputId": "f5ad1e17-f7db-4b04-9989-ad17dfa3bfb2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What light through yonder window breaks, Drawing thy soul to hear what I did never love my head And turn the throat, and therefore no more but such a wife, There are their faults to embrace, and weep no grace and virtue That did begin the seventh of them. But, if I hate, you would not miscontend me much. Thou gav’st thy hand, which cannot be a will. ARVIRAGUS. This is not true; the world’s earthly, And every man that discourse is a shaft between his father’s helm. Troy for’t. A goodly place and give me leave to call it heavy! PRINCESS. Live, That thou wilt swear to your confidence in the perilous clerk, Which, comfortable than the world to come, And you did love as this, but you shall hear. [_Exit._] MENENIUS. Peace, peace! Now plant thee company. Enter Talbot’s Majesty. PROTEUS. They are all abated. A lover I give, and so The glasses of my tongue Give me the edge of an enemy to my life. FALSTAFF. Come, come, neighbour, your bag-a-dog before A heavy head with strength on her sole bounty all as bitter there. The maidenheads,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = generate_text(\n",
        "    model=model,\n",
        "    start_string=\"What light through \",\n",
        "    char2idx=char2idx,\n",
        "    idx2char=idx2char,\n",
        "    num_generate=1000,\n",
        "    temperature=0.8  # Lower = more predictable, higher = more creative\n",
        ")\n",
        "\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlJPtWAfACdf",
        "outputId": "e98c43f9-cf06-4f65-f4ac-408671c2c92a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What light through y KANI MIDANIWEXAROLONONEx._] ACHENGOLALONGHENINININGERONUSECONIMEx._] IONGROCORYEx._PAVENIVOLANGROXLUREXUSERLINERY**********************************************************************************************************1 IORYENILE: ALORD.) THENOWINOLALORY. wave SACONGONILANÇONFRDUSAUSTHERDYONICOWhenghime._] WARY*********************************** IVALLALATHExeshevithinghig theshilousthe, thar wavis; wild ASTHEWALLALDUSHERONONISCUSTHEx SACORV”5_] m.” t thanghe._] way MOLUSTHENINILLONÇOLLATHERY._] NISENIONOLANIVANGONGONILLELANONI whe?_] ICANO._ERDUSExiz: LANEFIVICOWELLORDARIO, owe85 wizèSTHERO._] THEWAGRONGEBONIOLLONISTLYESTHUIVEBENExcheg! ALALLDIVIVERONGONGEx, he, THENINONICOWALLANINGONCLALSHENICOBELACONDENGHENILUSHEx THEDADALURLNGONÇONENANEx, thinghavinghenil WENISCENENIUSHENIORDALLIVEx._] LARY.” way? prdspld!”œ chizèd muld MALIONGHExe; MOWI THENGHUSILAUCONINI_] f ANIOLLACENILLAVICOLANGONOVAYONGRYOGEx ALIVExesby.” thevechere ACONICONICKICONGUSHANGUSANENIVENILLYONGHENGOWhare wisp wish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated = generate_text(\n",
        "    model=model,\n",
        "    start_string=\"i love her\",\n",
        "    char2idx=char2idx,\n",
        "    idx2char=idx2char,\n",
        "    num_generate=1000,\n",
        "    temperature=0.8  # Lower = more predictable, higher = more creative\n",
        ")\n",
        "\n",
        "print(generated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giy_4_DjAY_3",
        "outputId": "8b3a0b28-47ab-4b01-d39d-0ad91e6825c3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love her! CONGEX._] watheng; bysthe winghe wixenghed!” d MONGERDATHExigavequ ALATHELUSCERORLOLANICONGERY._ONGRY”) THExprethe? wathy, thavemaraclove8 SCONGONEXEROWAUFRENE._] t sothe ANGONISHENILUSHENI.” Whemize; KINGENIVENINÇONÇONUSCOXENICOLOLAFRY.” chabjERILALLEBESTHENIVERUSHALALONGExing, ANIONGENIOLALENIVEXERY CONILUSHUSHEx akere t k!_AMANGONICAMENIATHORDRATHALONROLOLOLALIONEKèd ALALLLLLUSCOLARORONIONÇENI THENGLALLUSENINWALOPHERI TERDECONÇOWADIOLURORDOLLOWESENINIVENIONENULURUSTHENIDUSEXEx; wake; musthowhequghachet, ivesthak’s. che fONGONIORONIONIVO2] TONGHUSCANULONGENÇONGUSTHERYONDANILIOVEXALORONÇONILONGLANIOLAGORYONTHERO: why, FIVIOLOXENGHESHETHELLSWIVECONGEXAVENICANICODAGONERY. VRONICKIONILONENXENINVADALONORA…” ICANIONESANEXTHUCOLLORONILADExure—flà” BALLY***************************** THURIVENGLUMO. ghœ feÇESTHExiz] bene—LANONIORDGORLANICOLLONOXENENGLANGOLOLANUSTLAN6 WIONVENONILICAFONIOCLONILLURDANILOWANIONI WADENÇOLOUCOLY.) THERDExesthevesure. ANILUSExet mangouxeÉ. wourard BEx THENOPHENIDæ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload text and mappings\n",
        "def clean_text(text):\n",
        "    text = text.replace('\\r', ' ')\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = clean_text(f.read())\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Dataset parameters\n",
        "SEQ_LENGTH = 100\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Build dataset\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(SEQ_LENGTH + 1, drop_remainder=True)\n",
        "dataset = sequences.map(split_input_target)\n",
        "full_dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Split into training and validation\n",
        "dataset_size = len(list(full_dataset))\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_dataset = full_dataset.skip(train_size).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "yojo16_A-nNO"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005, clipnorm=1.0),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "1CyxdHGd_DN9"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rebuild the model for flexible batch size\n",
        "model = build_shakespeare_model(batch_size=None)  # Not stateful\n",
        "model.load_weights('/content/drive/MyDrive/best_model.weights.h5')\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005, clipnorm=1.0),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "loss, accuracy = model.evaluate(val_dataset)\n",
        "print(f\"✅ Validation Loss: {loss:.4f}\")\n",
        "print(f\"✅ Validation Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU7O-3iN_mMY",
        "outputId": "c9e2bba3-cbab-4ef0-82e8-b922fbcc5516"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 168ms/step - accuracy: 0.6272 - loss: 1.1987\n",
            "✅ Validation Loss: 1.2009\n",
            "✅ Validation Accuracy: 0.6264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Character vocabulary (recreate from training)\n",
        "vocab = sorted(list(set(\n",
        "    ' !\"#$%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_`abcdefghijklmnopqrstuvwxyz{|}~\\n\\t'\n",
        ")))  # Adjust this if needed\n",
        "\n",
        "char2idx = {u: i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# 2. Convert input text to integer sequence\n",
        "def text_to_input_tensor(text):\n",
        "    input_ids = [char2idx[c] for c in text if c in char2idx]\n",
        "    return tf.expand_dims(input_ids, 0)  # shape (1, len)\n",
        "\n",
        "# 3. Get model prediction\n",
        "def predict_next_char(model, input_text, temperature=1.0):\n",
        "    input_tensor = text_to_input_tensor(input_text)\n",
        "    prediction = model(input_tensor)\n",
        "    prediction = prediction[:, -1, :]  # last character's prediction\n",
        "    prediction = prediction / temperature\n",
        "    predicted_id = tf.random.categorical(prediction, num_samples=1)[-1, 0].numpy()\n",
        "    return idx2char[predicted_id]\n",
        "\n",
        "# 4. Generate text from seed\n",
        "def generate_text(model, start_string, num_generate=300, temperature=1.0):\n",
        "    input_eval = [char2idx[c] for c in start_string if c in char2idx]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = list(start_string)\n",
        "\n",
        "    model.reset_states()\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[:, -1, :] / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return ''.join(text_generated)\n"
      ],
      "metadata": {
        "id": "f-bAaP1Z6e88"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Next char after 'ROMEO':\", predict_next_char(model, \"ROMEO\"))\n",
        "\n",
        "generated_text = generate_text(model, \"ROMEO: \", num_generate=400, temperature=0.8)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "tUWykAM76s1M",
        "outputId": "1a01341e-0cff-4ecd-c059-d124d2dd63d8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next char after 'ROMEO': Q\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Sequential' object has no attribute 'reset_states'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2967837555.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Next char after 'ROMEO':\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_next_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ROMEO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ROMEO: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_generate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-698473065.py\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(model, start_string, num_generate, temperature)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtext_generated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'reset_states'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MX7lpV6p6uh_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}