{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IB-9BZVuzHNO",
        "outputId": "85c04f26-43c7-4361-b0f3-817d1bc6bc49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bEaiYMQuzVVF"
      },
      "outputs": [],
      "source": [
        "weights_path = '/content/drive/MyDrive/best_model.weights.h5'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA2PJkNeyQKQ",
        "outputId": "496cfc52-20bf-4cfc-f0cb-882b316176a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def build_shakespeare_model(batch_size=None):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.InputLayer(input_shape=(None,), batch_size=batch_size),\n",
        "        tf.keras.layers.Embedding(input_dim=98, output_dim=384),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.GRU(1536, return_sequences=True,\n",
        "                          dropout=0.3, recurrent_dropout=0.3,\n",
        "                          stateful=batch_size is not None),\n",
        "        tf.keras.layers.GRU(768, return_sequences=True,\n",
        "                          dropout=0.2,\n",
        "                          stateful=batch_size is not None),\n",
        "        tf.keras.layers.Dense(98)\n",
        "    ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GPiwPgN144OW"
      },
      "outputs": [],
      "source": [
        "# Initialize for inference (batch_size=1)\n",
        "model = build_shakespeare_model(batch_size=1)\n",
        "model.load_weights(weights_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWEK6Mdi47Cr",
        "outputId": "67374bb5-c5b3-4ae3-b0dd-2be972829985"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test output shape: (1, 10, 98)\n"
          ]
        }
      ],
      "source": [
        "# Create test input matching your vocabulary\n",
        "# Using actual character IDs from your mappings:\n",
        "test_input = tf.constant([[6, 6, 6, 0, 41, 42, 23, 40, 42, 0]])  # From your sample conversion\n",
        "print(\"Test output shape:\", model(test_input).shape)  # Should output (1, 10, 98)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LiruaQ-4-7M",
        "outputId": "3ea067df-2ada-440b-e516-5d2b4c412f85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save('/content/drive/MyDrive/shakespeare_model.keras')  # New .keras format\n",
        "\n",
        "# 5. (Optional) Save in HDF5 format if needed\n",
        "model.save('/content/drive/MyDrive/shakespeare_model.h5')  # Legacy format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUARzfRk5pXC",
        "outputId": "14afeb54-362f-4cb2-9fc3-300c86bf10ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 10, 98)\n"
          ]
        }
      ],
      "source": [
        "# Load the saved model to verify\n",
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/shakespeare_model.keras')\n",
        "\n",
        "# Test with sample input\n",
        "test_input = tf.constant([[6, 6, 6, 0, 41, 42, 23, 40, 42, 0]])\n",
        "print(loaded_model(test_input).shape)  # Should output (1, 10, 98)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1M2PUMk9GgY",
        "outputId": "ea65ff49-539c-4144-f8ee-03aa83547b29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available devices:\n",
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 15266377498696355182\n",
            "xla_global_id: -1\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14619377664\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 5401738242019881787\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "xla_global_id: 416903419\n",
            "]\n",
            "Using strategy: _DefaultDistributionStrategy\n",
            "--2025-08-02 18:04:00--  https://www.gutenberg.org/files/100/100-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5618733 (5.4M) [text/plain]\n",
            "Saving to: ‘data/shakespeare.txt’\n",
            "\n",
            "data/shakespeare.tx 100%[===================>]   5.36M  9.07MB/s    in 0.6s    \n",
            "\n",
            "2025-08-02 18:04:01 (9.07 MB/s) - ‘data/shakespeare.txt’ saved [5618733/5618733]\n",
            "\n",
            "Loaded text length: 5297571 characters\n",
            "Vocabulary size: 98\n",
            "Sample mappings:\n",
            "' ': 0\n",
            "'!': 1\n",
            "'&': 2\n",
            "''': 3\n",
            "'(': 4\n",
            "Text as integers shape: (5297571,)\n",
            "Sample conversion: [ 6  6  6  0 41 42 23 40 42  0]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# Check available accelerators\n",
        "print(\"Available devices:\")\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "# Set up multi-GPU if available\n",
        "strategy = tf.distribute.MirroredStrategy() if len(tf.config.list_physical_devices('GPU')) > 1 else tf.distribute.get_strategy()\n",
        "print(f\"Using strategy: {strategy.__class__.__name__}\")\n",
        "\n",
        "# Download Shakespeare dataset\n",
        "!mkdir -p data\n",
        "!wget https://www.gutenberg.org/files/100/100-0.txt -O data/shakespeare.txt\n",
        "def clean_text(text):\n",
        "    \"\"\"More careful cleaning that maintains consistency\"\"\"\n",
        "    text = text.replace('\\r', ' ')  # Replace with space instead of removing\n",
        "    text = text.replace('\\n', ' ')  # Replace with space instead of removing\n",
        "    text = ' '.join(text.split())  # Normalize spaces/\n",
        "    return text\n",
        "\n",
        "# 1. Load and clean text\n",
        "with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = clean_text(f.read())\n",
        "print(f\"Loaded text length: {len(text)} characters\")\n",
        "\n",
        "# 2. Create vocabulary from CLEANED text\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(\"Sample mappings:\")\n",
        "for char in list(vocab)[:5]:\n",
        "    print(f\"'{char}': {char2idx[char]}\")\n",
        "\n",
        "# 3. Convert text to integers\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "print(f\"Text as integers shape: {text_as_int.shape}\")\n",
        "print(\"Sample conversion:\", text_as_int[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "80189bOQ4h6u"
      },
      "outputs": [],
      "source": [
        "# Load and clean text again\n",
        "def clean_text(text):\n",
        "    text = text.replace('\\r', ' ')\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "with open('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n",
        "    text = clean_text(f.read())\n",
        "\n",
        "\n",
        "# Recreate vocab\n",
        "vocab = sorted(set(text))\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "vocab_size = len(vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "e24y_qR7907M"
      },
      "outputs": [],
      "source": [
        "# Define this once in your notebook/script\n",
        "def reset_rnn_states(model):\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'reset_states'):\n",
        "            layer.reset_states()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8EQjcksx9gjC"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string, char2idx, idx2char, num_generate=1000, temperature=1.0):\n",
        "    input_eval = [char2idx[c] for c in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "    text_generated = []\n",
        "\n",
        "    reset_rnn_states(model)  # Reset RNN states properly\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = predictions[:, -1, :] / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNPl8QNL9jPb",
        "outputId": "f5ad1e17-f7db-4b04-9989-ad17dfa3bfb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What light through yonder window breaks, Drawing thy soul to hear what I did never love my head And turn the throat, and therefore no more but such a wife, There are their faults to embrace, and weep no grace and virtue That did begin the seventh of them. But, if I hate, you would not miscontend me much. Thou gav’st thy hand, which cannot be a will. ARVIRAGUS. This is not true; the world’s earthly, And every man that discourse is a shaft between his father’s helm. Troy for’t. A goodly place and give me leave to call it heavy! PRINCESS. Live, That thou wilt swear to your confidence in the perilous clerk, Which, comfortable than the world to come, And you did love as this, but you shall hear. [_Exit._] MENENIUS. Peace, peace! Now plant thee company. Enter Talbot’s Majesty. PROTEUS. They are all abated. A lover I give, and so The glasses of my tongue Give me the edge of an enemy to my life. FALSTAFF. Come, come, neighbour, your bag-a-dog before A heavy head with strength on her sole bounty all as bitter there. The maidenheads,\n"
          ]
        }
      ],
      "source": [
        "generated = generate_text(\n",
        "    model=model,\n",
        "    start_string=\"What light through yonder window breaks\",\n",
        "    char2idx=char2idx,\n",
        "    idx2char=idx2char,\n",
        "    num_generate=1000,\n",
        "    temperature=0.8  # Lower = more predictable, higher = more creative\n",
        ")\n",
        "\n",
        "print(generated)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX7lpV6p6uh_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
