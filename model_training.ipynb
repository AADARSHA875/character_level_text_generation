{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ‚úÖ Step 2: Install Dependencies\n!pip install tensorflow matplotlib\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SVB3bAacUDzw","outputId":"c999f6fc-0ac4-47e2-bf37-7193c70ece43","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://www.gutenberg.org/files/100/100-0.txt -O data/shakespeare.txt\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ooWI66NGUOYJ","outputId":"0b520cef-e22b-4691-bccb-68dd3155b6aa","trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:58:12.825406Z","iopub.execute_input":"2025-08-02T16:58:12.826291Z","iopub.status.idle":"2025-08-02T16:58:12.990036Z","shell.execute_reply.started":"2025-08-02T16:58:12.826263Z","shell.execute_reply":"2025-08-02T16:58:12.989081Z"}},"outputs":[{"name":"stdout","text":"data/shakespeare.txt: No such file or directory\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport os\nfrom tensorflow.python.client import device_lib\n\n# Check available accelerators\nprint(\"Available devices:\")\nprint(device_lib.list_local_devices())\n\n# Set up multi-GPU if available\nstrategy = tf.distribute.MirroredStrategy() if len(tf.config.list_physical_devices('GPU')) > 1 else tf.distribute.get_strategy()\nprint(f\"Using strategy: {strategy.__class__.__name__}\")\n\n# Download Shakespeare dataset\n!mkdir -p data\n!wget https://www.gutenberg.org/files/100/100-0.txt -O data/shakespeare.txt\ndef clean_text(text):\n    \"\"\"More careful cleaning that maintains consistency\"\"\"\n    text = text.replace('\\r', ' ')  # Replace with space instead of removing\n    text = text.replace('\\n', ' ')  # Replace with space instead of removing\n    text = ' '.join(text.split())  # Normalize spaces/\n    return text\n\n# 1. Load and clean text\nwith open('data/shakespeare.txt', 'r', encoding='utf-8') as f:\n    text = clean_text(f.read())\nprint(f\"Loaded text length: {len(text)} characters\")\n\n# 2. Create vocabulary from CLEANED text\nvocab = sorted(set(text))\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\nvocab_size = len(vocab)\n\nprint(f\"Vocabulary size: {len(vocab)}\")\nprint(\"Sample mappings:\")\nfor char in list(vocab)[:5]:\n    print(f\"'{char}': {char2idx[char]}\")\n\n# 3. Convert text to integers\ntext_as_int = np.array([char2idx[c] for c in text])\nprint(f\"Text as integers shape: {text_as_int.shape}\")\nprint(\"Sample conversion:\", text_as_int[:10])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ujVPgAPtO20l","outputId":"aa825c29-78bd-45c9-cacb-b0bc2874cc40","trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:59:01.198429Z","iopub.execute_input":"2025-08-02T16:59:01.199149Z","iopub.status.idle":"2025-08-02T16:59:02.743281Z","shell.execute_reply.started":"2025-08-02T16:59:01.199122Z","shell.execute_reply":"2025-08-02T16:59:02.742458Z"}},"outputs":[{"name":"stdout","text":"Available devices:\n[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 17527652512067048116\nxla_global_id: -1\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 14619377664\nlocality {\n  bus_id: 1\n  links {\n    link {\n      device_id: 1\n      type: \"StreamExecutor\"\n      strength: 1\n    }\n  }\n}\nincarnation: 7710084416329267826\nphysical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\nxla_global_id: 416903419\n, name: \"/device:GPU:1\"\ndevice_type: \"GPU\"\nmemory_limit: 14619377664\nlocality {\n  bus_id: 1\n  links {\n    link {\n      type: \"StreamExecutor\"\n      strength: 1\n    }\n  }\n}\nincarnation: 184280172459430724\nphysical_device_desc: \"device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\"\nxla_global_id: 2144165316\n]\nUsing strategy: MirroredStrategy\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1754153941.208598      36 gpu_device.cc:2022] Created device /device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1754153941.208820      36 gpu_device.cc:2022] Created device /device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"--2025-08-02 16:59:01--  https://www.gutenberg.org/files/100/100-0.txt\nResolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\nConnecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 5618733 (5.4M) [text/plain]\nSaving to: ‚Äòdata/shakespeare.txt‚Äô\n\ndata/shakespeare.tx 100%[===================>]   5.36M  20.8MB/s    in 0.3s    \n\n2025-08-02 16:59:01 (20.8 MB/s) - ‚Äòdata/shakespeare.txt‚Äô saved [5618733/5618733]\n\nLoaded text length: 5297571 characters\nVocabulary size: 98\nSample mappings:\n' ': 0\n'!': 1\n'&': 2\n''': 3\n'(': 4\nText as integers shape: (5297571,)\nSample conversion: [ 6  6  6  0 41 42 23 40 42  0]\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Create dataset\nSEQ_LENGTH = 100\nBATCH_SIZE = 128\nBUFFER_SIZE = 10000\n\ndef split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\n# Create and split dataset\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\nsequences = char_dataset.batch(SEQ_LENGTH+1, drop_remainder=True)\ndataset = sequences.map(split_input_target)\n\n# Shuffle and batch\nfull_dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n\n# Train/validation split\ndataset_size = len(list(full_dataset))\ntrain_size = int(0.8 * dataset_size)\nval_size = dataset_size - train_size\n\ntrain_dataset = full_dataset.take(train_size)\nval_dataset = full_dataset.skip(train_size)\n\n# In your dataset pipeline, add:\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\nval_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n\n# Expected effect:\n# - Batch time drops from 475ms ‚Üí ~350ms\n# - Full training time: ~4.4 hours (instead of 6.3)\n\nprint(f\"Training batches: {len(list(train_dataset))}, Validation batches: {len(list(val_dataset))}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-05XJ0KoO57k","outputId":"041cb950-6320-41c9-e1e5-0097e2773391","trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:59:08.793653Z","iopub.execute_input":"2025-08-02T16:59:08.794563Z","iopub.status.idle":"2025-08-02T16:59:32.968135Z","shell.execute_reply.started":"2025-08-02T16:59:08.794524Z","shell.execute_reply":"2025-08-02T16:59:32.967482Z"}},"outputs":[{"name":"stdout","text":"Training batches: 327, Validation batches: 82\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def build_improved_model(vocab_size, batch_size=None):\n    model = tf.keras.Sequential()\n    \n    # Input layer with proper batch handling\n    model.add(tf.keras.layers.InputLayer(\n        input_shape=(None,),\n        batch_size=batch_size))\n    \n    # Enhanced embedding layer\n    model.add(tf.keras.layers.Embedding(\n        input_dim=vocab_size,\n        output_dim=384))  # Increased from 256 to 384\n    \n    # Batch normalization for stability\n    model.add(tf.keras.layers.BatchNormalization())\n    \n    # First GRU layer with increased capacity\n    model.add(tf.keras.layers.GRU(\n        1536,  # Increased from 1024\n        return_sequences=True,\n        stateful=(batch_size is not None),\n        dropout=0.3,  # Slightly higher dropout\n        recurrent_dropout=0.3,\n        recurrent_initializer='glorot_uniform'))\n    \n    # Second GRU layer\n    model.add(tf.keras.layers.GRU(\n        768,  # Increased from 512\n        return_sequences=True,\n        stateful=(batch_size is not None),\n        dropout=0.2))\n    \n    # Output layer with softmax for better probability distribution\n    model.add(tf.keras.layers.Dense(\n        vocab_size,\n        activation='softmax'))  # Changed to softmax\n    \n    return model\n","metadata":{"id":"_3sUFr-8UZPJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"43129dff-cd90-4469-8891-85181dae7d8c","trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:59:32.969242Z","iopub.execute_input":"2025-08-02T16:59:32.969458Z","iopub.status.idle":"2025-08-02T16:59:32.976227Z","shell.execute_reply.started":"2025-08-02T16:59:32.969442Z","shell.execute_reply":"2025-08-02T16:59:32.975532Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import losses\n\n# 1. Locate the last checkpoint in Kaggle's working directory\ncheckpoint_dir = '/kaggle/working/training_checkpoints/'\nlatest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n\n# 2. Rebuild the model architecture (important!)\nmodel = build_improved_model(vocab_size, BATCH_SIZE)\n\n# 3. Load the weights\nif latest_checkpoint:\n    print(f\"Resuming from {latest_checkpoint}\")\n    model.load_weights(latest_checkpoint)\n    \n    # Extract last completed epoch number\n    initial_epoch = int(latest_checkpoint.split('_')[-1].split('.')[0])\nelse:\n    initial_epoch = 0\n    print(\"No checkpoint found, starting from scratch\")\n\n# 4. Recompile with same settings\nmodel.compile(\n    optimizer=optimizers.Adam(\n        learning_rate=0.0005,\n        clipnorm=1.0\n    ),\n    loss=losses.SparseCategoricalCrossentropy(from_logits=True),  # Common for text generation\n    metrics=['accuracy']\n)\n\n# 5. Update callbacks (modified to avoid overwriting)\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Corrected callback with proper file extension\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=os.path.join(checkpoint_dir, 'ckpt_{epoch:03d}.weights.h5'),  # Added .weights\n    monitor='val_loss',\n    save_weights_only=True,\n    save_best_only=False,\n    mode='min',\n    verbose=1\n)\n\n# Best model saver (also corrected)\nbest_model_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=os.path.join(checkpoint_dir, 'best_model.weights.h5'),  # Added .weights\n    monitor='val_loss',\n    save_weights_only=True,\n    save_best_only=True,\n    mode='min',\n    verbose=1\n)\n\n# 6. Continue training\nhistory = model.fit(\n    train_dataset.prefetch(tf.data.AUTOTUNE),\n    initial_epoch=initial_epoch,  # Critical for correct logging\n    epochs=50,  # Total epochs (original target)\n    validation_data=val_dataset.prefetch(tf.data.AUTOTUNE),\n    callbacks=[\n        checkpoint_callback,\n        best_model_callback,\n        tf.keras.callbacks.EarlyStopping(patience=10),\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss',\n            factor=0.5,\n            patience=3)\n    ]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:58:40.942160Z","iopub.execute_input":"2025-08-02T16:58:40.942410Z","iopub.status.idle":"2025-08-02T16:58:54.376954Z","shell.execute_reply.started":"2025-08-02T16:58:40.942395Z","shell.execute_reply":"2025-08-02T16:58:54.375974Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"No checkpoint found, starting from scratch\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/nn.py:708: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\nI0000 00:00:1754153933.050751     101 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1379048909.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# 6. Continue training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Critical for correct logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    917\u001b[0m           )\n\u001b[1;32m    918\u001b[0m       )\n\u001b[0;32m--> 919\u001b[0;31m       return self._concrete_variable_creation_fn._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    920\u001b[0m           \u001b[0mfiltered_flat_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concrete_variable_creation_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import os\n!ls -la /kaggle/working/training_checkpoints/  # Verify files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T16:50:20.885526Z","iopub.execute_input":"2025-08-02T16:50:20.885821Z","iopub.status.idle":"2025-08-02T16:50:21.005772Z","shell.execute_reply.started":"2025-08-02T16:50:20.885792Z","shell.execute_reply":"2025-08-02T16:50:21.005120Z"}},"outputs":[{"name":"stdout","text":"ls: cannot access '/kaggle/working/training_checkpoints/': No such file or directory\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Save the entire model (HDF5 format)\nmodel.save('/kaggle/working/full_model.h5') \n\n# OR save as SavedModel format (better for deployment)\nmodel.save('/kaggle/working/text_generator_model')  # Creates a directory","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-02T16:47:50.861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# To make files persist beyond session end:\nfrom IPython.display import FileLink\nFileLink('shakespeare_model.zip')  # Click to download\n\n# Or save to Kaggle datasets:\n!mkdir -p /kaggle/working/model_assets\n!cp -r /kaggle/working/saved_model /kaggle/working/model_assets/\n!kaggle datasets init -p /kaggle/working/model_assets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Manually stop the training cell (click stop button)\n\n# 2. Immediately save the model\nmodel.save_weights('/content/drive/MyDrive/interrupted_model_epoch40.weights.h5')\n\n# 3. Save training history\nimport pickle\nwith open('/content/drive/MyDrive/training_history.pkl', 'wb') as f:\n    pickle.dump(history.history, f)","metadata":{"id":"wTKS5C80z8Be","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Rebuild the same architecture\nnew_model = build_improved_model(vocab_size, batch_size=BATCH_SIZE)\n\n# 2. Load weights\nnew_model.load_weights('/content/drive/MyDrive/interrupted_model_epoch40.weights.h5')\n\n# 3. Continue training\nhistory = new_model.fit(\n    train_dataset,\n    initial_epoch=40,  # Critical: Start from epoch 41\n    epochs=60,         # New total epochs\n    validation_data=val_dataset,\n    callbacks=[...]    # Same callbacks\n)","metadata":{"id":"f8HtHlrSz-51","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Rebuild model for generation (batch_size=1)\ngen_model = build_improved_model(vocab_size, batch_size=1)\ngen_model.load_weights('/content/char-textgen/training_checkpoints/ckpt_14.weights.h5')\n\n# Generation function\ndef generate_text(model, start_string, temperature=0.7, num_generate=500):\n    input_indices = [char2idx[s] for s in start_string]\n    input_indices = tf.expand_dims(input_indices, 0)\n\n    text_generated = []\n    model.reset_states()\n\n    for _ in range(num_generate):\n        predictions = model(input_indices)\n        predictions = tf.squeeze(predictions, 0)\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        input_indices = tf.expand_dims([predicted_id], 0)\n        text_generated.append(idx2char[predicted_id])\n\n    return start_string + ''.join(text_generated)\n\n# Example generation\nprint(generate_text(gen_model, \"ROMEO:\", temperature=0.6))","metadata":{"id":"Sn7iNgRiUfHx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Visualization\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train')\nplt.plot(history.history['val_loss'], label='Validation')\nplt.title('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['accuracy'], label='Train')\nplt.plot(history.history['val_accuracy'], label='Validation')\nplt.title('Accuracy')\nplt.legend()\n\nplt.show()","metadata":{"id":"GXZqxdpeD7gq","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot training and validation loss\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['loss'], label='train_loss')\nplt.plot(history.history['val_loss'], label='val_loss')\nplt.legend()\nplt.title('Loss Curve')\nplt.show()\n","metadata":{"id":"MuKwdRaQR3zg","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the final trained model weights\nmodel_save_path = '/content/drive/MyDrive/shakespeare_gru_model.weights.h5'\nmodel.save_weights(model_save_path)\n\n# Save vocabulary mappings (essential for generation)\nimport pickle\nwith open('/content/drive/MyDrive/shakespeare_vocab.pkl', 'wb') as f:\n    pickle.dump({'char2idx': char2idx, 'idx2char': idx2char}, f)\n\nprint(f\"Model saved to: {model_save_path}\")\nprint(\"Vocabulary mappings saved\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K-wFQvnWDnmI","outputId":"3507200e-6b65-4a22-e554-1b09c7de7285","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text(model, start_string, temperature=0.7, num_generate=500):\n    # Vectorize the start string\n    input_indices = [char2idx[s] for s in start_string]\n    input_indices = tf.expand_dims(input_indices, 0)\n\n    text_generated = []\n    model.reset_states()\n\n    for _ in range(num_generate):\n        predictions = model(input_indices)\n        # Remove batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # Apply temperature scaling\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # Append predicted character and update input\n        text_generated.append(idx2char[predicted_id])\n        input_indices = tf.expand_dims([predicted_id], 0)\n\n    return start_string + ''.join(text_generated)","metadata":{"id":"ODNlkNQcR5Jj","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nwith open('/content/drive/MyDrive/shakespeare_vocab.pkl', 'rb') as f:\n    vocab_data = pickle.load(f)\n\nchar2idx = vocab_data['char2idx']\nidx2char = vocab_data['idx2char']\nvocab_size = len(idx2char)\n","metadata":{"id":"Zjho-XsPafE7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_dim = 256\nrnn_units = 1024\n\nmodel = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\nmodel.load_weights('/content/drive/MyDrive/shakespeare_gru_model.weights.h5')\nmodel.build(tf.TensorShape([1, None]))\n","metadata":{"id":"O9xixehxbHhQ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text(model, start_string, char2idx, idx2char, temperature=1.0, num_generate=300):\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    text_generated = []\n\n    # üîÅ Reset RNN states\n    for layer in model.layers:\n        if hasattr(layer, 'reset_states'):\n            layer.reset_states()\n\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        predictions = tf.squeeze(predictions, 0)\n\n        predictions = predictions / temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n\n        input_eval = tf.expand_dims([predicted_id], 0)\n        text_generated.append(idx2char[predicted_id])\n\n    return start_string + ''.join(text_generated)\n","metadata":{"id":"sJB4tTKebOn4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(generate_text(\n    model,\n    start_string=\"once upon a time\",\n    char2idx=char2idx,\n    idx2char=idx2char,\n    temperature=1.5\n))\n\n\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o0mhKHUXbeRw","outputId":"4d59b21f-72e5-4afc-9f31-e60055bf4f67","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example 1: Romantic opening\nprint(generate_text(model, \"ROMEO:\", temperature=0.7))\n\n# Example 2: Philosophical\nprint(generate_text(model, \"To be or not to be\", temperature=0.5))\n\n# Example 3: Comedy\nprint(generate_text(model, \"Fool:\", temperature=1.0))\n\n# Example 4: Custom input\nyour_text = input(\"Enter starting text: \")\nprint(generate_text(model, your_text, temperature=0.8))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"hfWcljyRR6rA","outputId":"d5d20853-f387-4236-e1e8-aa9f215d8bc1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(generate_text(model, start_string=\"she was waiting me\", char2idx=char2idx, idx2char=idx2char))\n","metadata":{"id":"x1HexqwoaYP5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"6fcThXxSbs4i","trusted":true},"outputs":[],"execution_count":null}]}